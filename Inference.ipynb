{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA1lW9pzWwpk",
        "outputId": "c3c126b8-7e4c-4a76-ea56-f6e6a060a257"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "zlpjJOhtW7g3"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 8192\n",
        "dtype = None\n",
        "load_in_4bit = True \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "5GxOyBTkXJIG",
        "outputId": "61eeaa7c-83ca-4e21-8dc0-520d1e583c9c"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qcAMetyBOIX",
        "outputId": "982c0b4c-ce55-4194-f934-48b252b9fdc5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('ibm/finqa')\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKt3vZoSeRvb"
      },
      "source": [
        "## Loading model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnuFw_ySq4uK"
      },
      "outputs": [],
      "source": [
        "!unzip model2.zip -d /content/model2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paHfJLfVccmN",
        "outputId": "8d6d1a12-f598-498c-a428-312a0b8d0f22"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"/content/drive/MyDrive/DL_Models/model2\", \n",
        "        max_seq_length = 8192,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "262c6VUJBxjG",
        "outputId": "fbb108bc-cc0b-48f8-f5c5-055b64bec48f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtzPvlZTJ43p",
        "outputId": "a7f42e2a-242b-4955-8567-71bfb1565161"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset['test']\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V4zEPUCCDiuD"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"You are an expert in the financial domain tasked with solving a given financial problem. Follow these steps:\n",
        "\n",
        "1. Read the question carefully.\n",
        "2. Provide the formula for the given question.\n",
        "3. Use the formula to caculate the final answer.\n",
        "\n",
        "\n",
        "### Pretext:\n",
        "{}\n",
        "\n",
        "### Posttext:\n",
        "{}\n",
        "\n",
        "### Table:\n",
        "{}\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm-8gQpZJlck",
        "outputId": "c7e2c02b-d209-4507-e3e4-299d8cba3496"
      },
      "outputs": [],
      "source": [
        "final_response = []\n",
        "for i in range(len(test_dataset)):\n",
        "  FastLanguageModel.for_inference(model)\n",
        "  pre_text = test_dataset[i][\"pre_text\"]\n",
        "  post_text = test_dataset[i][\"post_text\"]\n",
        "  table = test_dataset[i][\"table\"]\n",
        "  question = test_dataset[i][\"question\"]\n",
        "  input_prompt = prompt.format(\n",
        "          pre_text,\n",
        "          post_text,\n",
        "          table,\n",
        "          question,\n",
        "          #\"\",\n",
        "          # program_re,\n",
        "          # gold_inds,\n",
        "          \"\", \n",
        "      )\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "      input_prompt\n",
        "  ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "  input_shape = inputs['input_ids'].shape\n",
        "  input_token_len = input_shape[1]\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "\n",
        "  response = tokenizer.batch_decode([outputs[0][input_token_len:]], skip_special_tokens=True)\n",
        "  final_response.append(response[0])\n",
        "  if i%100 == 0:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgmQlsDONhaR",
        "outputId": "2d086ed6-778b-4daa-8a27-be167ae22626"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def parse_number(string):\n",
        "    \"\"\"\n",
        "    Extracts numerical values (integers, floats, or percentages) from a string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        clean_string = re.sub(r'[^\\d.%\\-]', '', string)\n",
        "\n",
        "        if clean_string.endswith('%'):\n",
        "            return float(clean_string.strip('%')) / 100  \n",
        "        return float(clean_string) \n",
        "    except ValueError:\n",
        "        return None  \n",
        "\n",
        "count = 0\n",
        "tolerance = 0.00  \n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    answer = test_dataset[i][\"answer\"]\n",
        "    predicted = final_response[i]\n",
        "\n",
        "    answer_value = parse_number(answer)\n",
        "    predicted_value = parse_number(predicted)\n",
        "\n",
        "    if answer_value is not None and predicted_value is not None:\n",
        "        if abs(answer_value - predicted_value) <= tolerance * abs(answer_value):\n",
        "            count += 1\n",
        "    else:\n",
        "        if predicted.strip().lower() == answer.strip().lower():\n",
        "            count += 1\n",
        "\n",
        "print(f\"Correct predictions within margin: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Td9nY7kNo-2",
        "outputId": "390f8a69-367f-4210-9c69-02040e34d981"
      },
      "outputs": [],
      "source": [
        "print(count/len(test_dataset)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zuCccehQL5W"
      },
      "source": [
        "## Few Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVe4DnmlPEx3",
        "outputId": "0708e0bd-4206-44a0-a8c3-7ad248d3a50a"
      },
      "outputs": [],
      "source": [
        "samples_prompt = \"\"\"These are the examples:\n",
        "\n",
        "### Pretext:\n",
        "{}\n",
        "\n",
        "### Posttext:\n",
        "{}\n",
        "\n",
        "### Table:\n",
        "{}\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "{}\n",
        "\n",
        "### Final_result:\n",
        "{}\n",
        "\n",
        "### Program_re:\n",
        "{}\n",
        "\n",
        "### GoldInds:\n",
        "{}\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "temp = np.array(dataset['train'])\n",
        "indices = np.random.choice(len(temp),5,replace=False)\n",
        "print(indices)\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token \n",
        "\n",
        "pre_text1 = []\n",
        "post_text1 = []\n",
        "table1 = []\n",
        "question1 = []\n",
        "answer1 = []\n",
        "final_result1 = []\n",
        "program_re1 = []\n",
        "gold_inds1 = []\n",
        "for i in indices:\n",
        "  pre_text1.append(dataset['train'][\"pre_text\"][int(i)])\n",
        "  post_text1.append(dataset['train'][\"post_text\"][int(i)])\n",
        "  table1.append(dataset['train'][\"table\"][int(i)])\n",
        "  question1.append(dataset['train'][\"question\"][int(i)])\n",
        "  answer1.append(dataset['train'][\"answer\"][int(i)])\n",
        "  final_result1.append(dataset['train'][\"final_result\"][int(i)])\n",
        "  program_re1.append(dataset['train'][\"program_re\"][int(i)])\n",
        "  gold_inds1.append(dataset['train'][\"gold_inds\"][int(i)])\n",
        "eg = []\n",
        "\n",
        "for a,b,c,d,e,f,g,h in zip(pre_text1,post_text1, table1, question1, answer1, final_result1, program_re1, gold_inds1):\n",
        "    sample = samples_prompt.format(a,b,c,d,e,f,g,h) + EOS_TOKEN\n",
        "    eg.append(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lP4uLUdpPZpe"
      },
      "outputs": [],
      "source": [
        "final_prompt = \"\"\"You are an expert in the financial domain tasked with solving a given financial problem. Follow these steps:\n",
        "\n",
        "1. Look at the examples to see how similar problems are solved.\n",
        "2. Read the question carefully.\n",
        "3. Provide the formula for the given question.\n",
        "4. Use the formula to caculate the final answer.\n",
        "\n",
        "### Examples:\n",
        "{}\n",
        "\n",
        "### Pretext:\n",
        "{}\n",
        "\n",
        "### Posttext:\n",
        "{}\n",
        "\n",
        "### Table:\n",
        "{}\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### GoldInds:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func2(examples):\n",
        "\n",
        "    pre_text = examples[\"pre_text\"]\n",
        "    post_text = examples[\"post_text\"]\n",
        "    table = examples[\"table\"]\n",
        "    question = examples[\"question\"]\n",
        "    answer = examples[\"answer\"]\n",
        "    final_result = examples[\"final_result\"]\n",
        "    program_re = examples[\"program_re\"]\n",
        "    gold_inds = examples[\"gold_inds\"]\n",
        "    output = examples[\"answer\"]\n",
        "\n",
        "    texts = []\n",
        "    for a,b,c,d,e,f,g,h in zip(pre_text,post_text, table, question, answer, program_re, gold_inds, output):\n",
        "        text = final_prompt.format(eg,a,b,c,d,g,h) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl4qyO3LPtbI",
        "outputId": "301adba4-1ceb-4742-c41d-8c9752725861"
      },
      "outputs": [],
      "source": [
        "final_response2 = []\n",
        "for i in range(len(test_dataset)):\n",
        "  FastLanguageModel.for_inference(model)\n",
        "  pre_text = test_dataset[i][\"pre_text\"]\n",
        "  post_text = test_dataset[i][\"post_text\"]\n",
        "  table = test_dataset[i][\"table\"]\n",
        "  question = test_dataset[i][\"question\"]\n",
        "  answer = \"\"\n",
        "  program_re = \"\"\n",
        "  gold_inds = \"\"\n",
        "  input_prompt = final_prompt.format(\n",
        "          eg,\n",
        "          pre_text,\n",
        "          post_text,\n",
        "          table,\n",
        "          question,\n",
        "          answer, \n",
        "          program_re,\n",
        "          gold_inds,\n",
        "          \"\", \n",
        "      )\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "      input_prompt\n",
        "  ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "  input_shape = inputs['input_ids'].shape\n",
        "  input_token_len = input_shape[1] \n",
        "  outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "\n",
        "  response = tokenizer.batch_decode([outputs[0][input_token_len:]], skip_special_tokens=True)\n",
        "  final_response2.append(response[0])\n",
        "  if i%100 == 0:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE1WKnAkRIRd",
        "outputId": "a2549130-461b-4006-bec2-9e9b94923424"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def parse_number(string):\n",
        "    \"\"\"\n",
        "    Extracts numerical values (integers, floats, or percentages) from a string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        clean_string = re.sub(r'[^\\d.%\\-]', '', string)\n",
        "\n",
        "        if clean_string.endswith('%'):\n",
        "            return float(clean_string.strip('%')) / 100 \n",
        "        return float(clean_string) \n",
        "    except ValueError:\n",
        "        return None  \n",
        "\n",
        "count = 0\n",
        "tolerance = 0.05  \n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    answer = test_dataset[i][\"answer\"]\n",
        "    predicted = final_response2[i]\n",
        "\n",
        "    answer_value = parse_number(answer)\n",
        "    predicted_value = parse_number(predicted)\n",
        "\n",
        "    if answer_value is not None and predicted_value is not None:\n",
        "\n",
        "        if abs(answer_value - predicted_value) <= tolerance * abs(answer_value):\n",
        "            count += 1\n",
        "    else:\n",
        "        if predicted.strip().lower() == answer.strip().lower():\n",
        "            count += 1\n",
        "\n",
        "print(f\"Correct predictions within margin: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ky_sKD5UXGz",
        "outputId": "5d57a6d9-66fc-4513-a976-a7871fd1f7f5"
      },
      "outputs": [],
      "source": [
        "print(count/len(test_dataset)*100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
